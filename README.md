# Semantic Segmentation
Implementation and Comparison of UNet with VGG-16, MobileNet-V2 and EfficientNet-B5 backbones.

## Contents
1. [Abstract](#abstract)
2. [Introduction](#introduction)
3. [Set Up Instructions](#set-up-instructions)
4. [Data Set](#dataset)
5. [Models](#models)
6. [Results](#results)

## Abstract
Semantic segmentation is the task of assigning a class label to each
pixel in an image, enabling detailed object localization and pixel-wise
understanding of the scene. In this assignment, UNet is implemented
using three backbones, which are VGG-16(baseline), MobileNet-V2,
and EfficientNet-B5. The results of the models are compared using
many different evaluation metrics (Accuracy, F1-Score, Dice, Specificity, Sensitivity).


## Introduction
Semantic segmentation is a fundamental computer vision task that aims to
assign a semantic label to every pixel in an image. It differs from traditional image classification, where a single label is assigned to the entire image. Semantic segmentation provides a more detailed understanding of the scene
by segmenting the image into meaningful regions, enabling fine-grained analysis and precise object localization. Its applications are widespread, including autonomous driving, medical imaging, and scene understanding.

To achieve accurate pixel-level predictions, semantic segmentation relies on
advanced deep learning models, often based on convolutional neural networks
(CNNs). These models learn to capture intricate spatial relationships and contextual information. They typically follow an encoder-decoder architecture, where the encoder extracts features and downsamples the image, while the
decoder upsamples and refines the features. The final output is a dense prediction map, where each pixel is assigned a class label or a probability distribution over multiple classes. To obtain the probability values, the feature map generated by the decoder is passed through a softmax layer, which normalizes the values to represent class probabilities.

## Set Up Instructions
To run these models, here are the instructions.

### Requirements
- python==3.10.11
- numpy==1.22.4
- pandas==1.5.3
- scikit-learn==1.2.2
- pytorch==2.0.0+cu118
- torchvision==0.15.1+cu118
- pillow==8.4.0
- matplotlib==3.5.1
- seaborn==0.12.2
- tqdm==4.65.0

### Data Set
The dataset can be requested from [here](https://drive.google.com/file/d/1dj2oG5y-D8kEOilE7EzNNoidfRef_eiM/view?usp=share_link). Please load the dataset on to your Google Drive from where it will be mounted.

### Running the Train Scripts
To train the model on your system, please follow the following steps:
1. Download the train script from the 'Train' folder and upload to Google Colab.
2. Mount the Google Drive and change the paths in the training notebook.
3. Run the code sequentially.
4. When training the model, one epoch takes one hour on standard Google Colab GPU T4. The training progress and the checkpoint at each epoch are saved as '.pth' file in the drive (keep in mind to change the paths).
5. After training is complete, the best validation performance model will be saved in your Google Drive.

### Running the Test Scripts
To test the model on the dataset, please follow the following steps:
1. From the 'Models' folder, open the models link.
2. Download the model you want to test.
3. Download the respective test script from the 'Test' folder.
4. Upload the test script and the model file to Google Colab.
5. Mount the Google Drive and change the paths in the testing notebook.
6. Run the code sequentially.

## DataSet
The dataset used in this project is quite small. It contained images of
real-life scenes from the streets mostly. There were 2 folders in the data, one
containing the train images and their corresponding masks and the other containing the test images and their corresponding masks. The train set was split
into train and validation with an 80-20 ratio. The data set has the following
properties:
• 367 images and masks in the train set.
• 101 images and masks in the test set, from the same scene.
• There are 12 semantic classes: Background, Sky, Building, Pole, Road,
Pavement, Tree, Sign Symbol, Fence, Car, Pedestrian, and Bicyclist.

In this project, the count of images in the train, test and validation set is as follows.
| Train | Test | Validation |
|-------|-------|-----------|
| 293 | 101 | 74 |

## Models
The model chosen for this task is UNet. It is implemented using three different backbones and the results are compared. The backbones are VGG-16, MobileNet-V2, and EfficientNet-B5. UNet with VGG-16 backbone is considered as the baseline.

The encoder of the UNet is determined by the backbone we select and the
decoder is such that it complements the backbone by gradually upsampling and
fusing the feature maps. 

<p align="center">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Architecture/unet%20architecture.png" width="200" height="500">
</p>

The paramters and the transfer learning setting for the models are as follows. Transfer learning is applied to all of the models, with the pre-trained weights from Imagenet.

|UNET with the backbone of |  |  | 
|-------|-----------|--------------|-----------------|
|       | VGG-16 (baseline) | MobileNet-V2 | EfficientNet-B5 |
|-------|-----------|--------------|-----------------|
| Parameters | 23,749,836 | 6,630,540 | 30,164,924 |
| Transfer Learning | Yes | Yes | Yes |

### Loss Functions
The loss function used Cross Entropy Loss.

### Training Parameters
|              UNET with the backbone of                     |
|       | VGG-16 (baseline) | MobileNet-V2 | EfficientNet-B5 |
|---------------|--------------|------------------|----------|
| Initial Learning Rate | 0.001 | 0.001 | 0.001 |
| Optimizer | Adam | Adam | Adam |
| Scheduler | StepLR | StepLR | StepLR |
| Scheduler Step Size | 30 | 30 | 30 |
| Scheduler gamma | 0.1 | 0.1 | 0.1 |
| Batch Size | 8 | 8 | 8 |
| Epochs | 50 | 50 | 50 |
| Time per Epoch (secs) | 12 | 6 | 15 |

### Training Graphs for UNet with EfficientNet-B5 backbone
<p align="center">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Training%20Graphs/effacc.png" width="200" height="200">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Training%20Graphs/effloss.png" width="200" height="200">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Training%20Graphs/effiou.png" width="200" height="200">
</p>

## Results
#### Quantitative Classification Results (Expression)

|                | **MobileNet V2** | **Efficient Net B1** | **ResNet18** |
| -------------- | ---------------- | --------------------- | ------------ |
| **Accuracy**         | 73.2             | 66.3                  | **76.7**          |
| **Cohens Kappa**     | 0.60             | 0.46                  | **0.66**          |
| **Krippendorffs Alpha** | -0.14            | -0.14                 | -0.14            |
| **AUC**              | 0.67             | 0.58                  | **0.72**          |
| **AUC-PR**           | 0.56             | 0.57                  | 0.56            |

The classification report:

|                  | Precision | Recall | F1-score | Support |
|------------------|-----------|--------|----------|---------|
| **MobileNet V2**    |           |        |          |           |
| Neutral          | 0.58      | 0.86   | 0.69     | 14852     |
| Happy            | 0.89      | 0.89   | 0.89     | 27005     |
| Sad              | 0.72      | 0.31   | 0.43     | 5013      |
| Surprise         | 0.57      | 0.37   | 0.45     | 2799      |
| Fear             | 0.63      | 0.16   | 0.26     | 1274      |
| Disgust          | 0.46      | 0.10   | 0.16     | 766       |
| Anger            | 0.61      | 0.49   | 0.55     | 5043      |
| Contempt         | 0.00      | 0.00   | 0.00     | 799       |
| **EfficientNet B1** |           |        |          |           |
| Neutral          | 0.50      | 0.83   | 0.62     | 14982     |
| Happy            | 0.79      | 0.95   | 0.86     | 26889     |
| Sad              | 0.91      | 0.01   | 0.02     | 4921      |
| Surprise         | 1.00      | 0.00   | 0.00     | 2871      |
| Fear             | 0.00      | 0.00   | 0.00     | 1341      |
| Disgust          | 0.00      | 0.00   | 0.00     | 775       |
| Anger            | 0.75      | 0.0.   | 0.06     | 4974      |
| Contempt         | 0.00      | 0.00   | 0.00     | 778       |
| **ResNet 18**         |           |        |          |           |
| Neutral          | 0.65      | 0.81   | 0.72     | 14837     |
| Happy            | 0.89      | 0.92   | 0.91     | 26935     |
| Sad              | 0.69      | 0.50   | 0.58     | 5083      |
| Surprise         | 0.61      | 0.42   | 0.50     | 2817      |
| Fear             | 0.60      | 0.37   | 0.46     | 1317      |
| Disgust          | 0.56      | 0.22   | 0.32     | 762       |
| Anger            | 0.66      | 0.58   | 0.62     | 5018      |
| Contempt         | 0.25      | 0.00   | 0.01     | 762       |

ResNet-18 has outperformed the other two models in all three tasks.

### Some classifications by ResNet-18
<p align="left">
    <img src="https://github.com/ReehaKhan/Facial-Expression-Recognition-and-Computing-Valence-and-Arousal/blob/main/Performance%20Evaluation/ResNet-18/res%20correctly%20classified%20images.png" width="500" height="300">
</p>
<p align="left">
    <img src="https://github.com/ReehaKhan/Facial-Expression-Recognition-and-Computing-Valence-and-Arousal/blob/main/Performance%20Evaluation/ResNet-18/res%20incorrectly%20classified%20images.png" width="500" height="300">
