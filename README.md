# Semantic Segmentation
Implementation and Comparison of UNet with VGG-16, MobileNet-V2 and EfficientNet-B5 backbones.

## Contents
1. [Abstract](#abstract)
2. [Introduction](#introduction)
3. [Set Up Instructions](#set-up-instructions)
4. [Data Set](#dataset)
5. [Models](#models)
6. [Results](#results)

## Abstract
Semantic segmentation is the task of assigning a class label to each
pixel in an image, enabling detailed object localization and pixel-wise
understanding of the scene. In this assignment, UNet is implemented
using three backbones, which are VGG-16(baseline), MobileNet-V2,
and EfficientNet-B5. The results of the models are compared using
many different evaluation metrics (Accuracy, F1-Score, Dice, Specificity, Sensitivity).


## Introduction
Semantic segmentation is a fundamental computer vision task that aims to
assign a semantic label to every pixel in an image. It differs from traditional image classification, where a single label is assigned to the entire image. Semantic segmentation provides a more detailed understanding of the scene
by segmenting the image into meaningful regions, enabling fine-grained analysis and precise object localization. Its applications are widespread, including autonomous driving, medical imaging, and scene understanding.

To achieve accurate pixel-level predictions, semantic segmentation relies on
advanced deep learning models, often based on convolutional neural networks
(CNNs). These models learn to capture intricate spatial relationships and contextual information. They typically follow an encoder-decoder architecture, where the encoder extracts features and downsamples the image, while the
decoder upsamples and refines the features. The final output is a dense prediction map, where each pixel is assigned a class label or a probability distribution over multiple classes. To obtain the probability values, the feature map generated by the decoder is passed through a softmax layer, which normalizes the values to represent class probabilities.

## Set Up Instructions
To run these models, here are the instructions.

### Requirements
- python==3.10.11
- numpy==1.22.4
- pandas==1.5.3
- scikit-learn==1.2.2
- pytorch==2.0.0+cu118
- torchvision==0.15.1+cu118
- pillow==8.4.0
- matplotlib==3.5.1
- seaborn==0.12.2
- tqdm==4.65.0

### Data Set
The dataset can be requested from [here](https://drive.google.com/file/d/1dj2oG5y-D8kEOilE7EzNNoidfRef_eiM/view?usp=share_link). Please load the dataset on to your Google Drive from where it will be mounted.

### Running the Train Scripts
To train the model on your system, please follow the following steps:
1. Download the train script from the 'Train' folder and upload to Google Colab.
2. Mount the Google Drive and change the paths in the training notebook.
3. Run the code sequentially.
4. When training the model, one epoch takes one hour on standard Google Colab GPU T4. The training progress and the checkpoint at each epoch are saved as '.pth' file in the drive (keep in mind to change the paths).
5. After training is complete, the best validation performance model will be saved in your Google Drive.

### Running the Test Scripts
To test the model on the dataset, please follow the following steps:
1. From the 'Models' folder, open the models link.
2. Download the model you want to test.
3. Download the respective test script from the 'Test' folder.
4. Upload the test script and the model file to Google Colab.
5. Mount the Google Drive and change the paths in the testing notebook.
6. Run the code sequentially.

## DataSet
The dataset used in this project is quite small. It contained images of
real-life scenes from the streets mostly. There were 2 folders in the data, one
containing the train images and their corresponding masks and the other containing the test images and their corresponding masks. The train set was split
into train and validation with an 80-20 ratio. The data set has the following
properties:
- 367 images and masks in the train set.
- 101 images and masks in the test set, from the same scene.
- There are 12 semantic classes: Background, Sky, Building, Pole, Road,
Pavement, Tree, Sign Symbol, Fence, Car, Pedestrian, and Bicyclist.

In this project, the count of images in the train, test and validation set is as follows.
| Train | Test | Validation |
|-------|-------|-----------|
| 293 | 101 | 74 |

## Data Augmentation
There is a class imbalance in the dataset, therefore,
we propose to apply more augmentation to the images that contain the
low-frequency classes. For this, 2 different transforms are defined:
- Transform: This is a basic transform that only resizes, and flips.
- Transform Augmentation: This is the transform that is used to apply the
augmentation. It contains many different augmentation ways.

The conditions to apply each of the
transforms are discussed in the following pipeline:
1. The percentage of each class in the train set is found.
2. Any class whose percentage is said to be a low-frequency class.
3. For each image in the training set, find the percentage of classes in that
image.
4. If the percentage of any of the low-frequency classes is greater than 10%
then apply the transform augmentation to that image and mask. If not,
then apply the simple transform.
5. For all the images and masks in the validation and testing set, the simple
transform is only applied.

<p align="center">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Augmentation/transform.png" width="600" height="50">
    <br>
    <em>Simple Transform</em>
    </br>
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Augmentation/aug_transform.png" width="600" height="300">
    <br>
    <em>Augmentation Transform</em>
    </br>
</p>

Along with these, normalization is also performed by first calculating the
mean and standard deviation of the train set. The details of the augmentation
transform are as follows:
- Resize the image to a size of 256x256 pixels using nearest neighbor
interpolation.
- Apply horizontal and vertical flips randomly.
- Perform random cropping of the image to a size of 256x256 pixels.
- Randomly rotate the image by 90 degrees.
- Apply grid distortion with a probability of 0.2.
- Apply one of the following transformations randomly: random brightness/-
contrast adjustment, random hue/saturation/value adjustment, or random
RGB channel shift.
- Add Gaussian noise to the image.

## Models
The model chosen for this task is UNet. It is implemented using three different backbones and the results are compared. The backbones are VGG-16, MobileNet-V2, and EfficientNet-B5. UNet with VGG-16 backbone is considered as the baseline.

The encoder of the UNet is determined by the backbone we select and the
decoder is such that it complements the backbone by gradually upsampling and
fusing the feature maps. 

<p align="center">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Architecture/unet%20architecture.png" width="600" height="400">
</p>

The paramters and the transfer learning setting for the models are as follows. Transfer learning is applied to all of the models, with the pre-trained weights from Imagenet.

|   UNet with the backbone of     | VGG-16 (baseline) | MobileNet-V2 | EfficientNet-B5 |
|-------|-----------|--------------|-----------------|
| Parameters | 23,749,836 | 6,630,540 | 30,164,924 |
| Transfer Learning | Yes | Yes | Yes |

### Loss Functions
The loss function used Cross Entropy Loss.

### Training Parameters
|   UNet with the backbone of     | VGG-16 (baseline) | MobileNet-V2 | EfficientNet-B5 |
|---------------|--------------|------------------|----------|
| Initial Learning Rate | 0.001 | 0.001 | 0.001 |
| Optimizer | Adam | Adam | Adam |
| Scheduler | StepLR | StepLR | StepLR |
| Scheduler Step Size | 30 | 30 | 30 |
| Scheduler gamma | 0.1 | 0.1 | 0.1 |
| Batch Size | 8 | 8 | 8 |
| Epochs | 50 | 50 | 50 |
| Time per Epoch (secs) | 12 | 6 | 15 |

### Training Graphs for UNet with EfficientNet-B5 backbone
<p align="center">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Training%20Graphs/effacc.png" width="250" height="250">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Training%20Graphs/effloss.png" width="250" height="250">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Training%20Graphs/effmoi.png" width="250" height="250">
</p>

## Results
#### Quantitative Classification Results (Expression)

|     U-Net with backbone of        | VGG16(baseline) | MobileNet-V2 | EfficientNetB5 |
|:-------------------------------:|---|---|---|
| **Accuracy**    | 90.7  |     90.9     |     **91.6**   |
| **Loss**        | 0.29  |     0.28     |      0.28     |
| **mIoU**        | 0.64  |     0.61     |      0.65     |


The classification report along with Dice, Sensitivity, and Specificity:

| **UNet with Backbone** | **Class** | **Precision** | **Recall** | **F1-score** | **Support** | **Dice** | **Sensitivity** | **Specificity** |
|------------------------|-----------|---------------|------------|--------------|-------------|----------|-----------------|-----------------|
| **VGG-16 (baseline)**  | 0         | 0.97          | 0.97       | 0.97         | 615,463     | 0.96     | 0.97            | 0.99            |
|                        | 1         | 0.94          | 0.87       | 0.91         | 1,717,184   | 0.90     | 0.86            | 0.98            |
|                        | 2         | 0.23          | 0.07       | 0.10         | 36,233      | 0.10     | 0.07            | 0.99            |
|                        | 3         | 0.99          | 0.97       | 0.98         | 1,902,580   | 0.98     | 0.97            | 0.99            |
|                        | 4         | 0.89          | 0.95       | 0.92         | 572,216     | 0.92     | 0.95            | 0.99            |
|                        | 5         | 0.92          | 0.97       | 0.94         | 1,096,160   | 0.94     | 0.97            | 0.98            |
|                        | 6         | 0.78          | 0.43       | 0.55         | 59,336      | 0.55     | 0.43            | 0.99            |
|                        | 7         | 0.65          | 0.77       | 0.71         | 203,187     | 0.71     | 0.77            | 0.99            |
|                        | 8         | 0.83          | 0.91       | 0.87         | 115,106     | 0.87     | 0.91            | 0.99            |
|                        | 9         | 0.33          | 0.64       | 0.43         | 43,080      | 0.43     | 0.64            | 0.99            |
|                        | 10        | 0.89          | 0.57       | 0.70         | 145,587     | 0.70     | 0.57            | 0.99            |
|                        | 11        | 0.27          | 0.43       | 0.33         | 113,004     | 0.33     | 0.43            | 0.97            |
| **MobileNet V2**       | 0         | 0.96          | 0.97       | 0.97         | 607,222     | 0.96     | 0.97            | 0.99            |
|                        | 1         | 0.94          | 0.89       | 0.91         | 1,717,184   | 0.91     | 0.89            | 0.97            |
|                        | 2         | 0.23          | 0.08       | 0.12         | 36,233      | 0.001    | 0.0008          | 1.00            |
|                        | 3         | 0.99          | 0.97       | 0.98         | 1,902,580   | 0.98     | 0.97            | 0.99            |
|                        | 4         | 0.90          | 0.96       | 0.92         | 572,216     | 0.92     | 0.96            | 0.99            |
|                        | 5         | 0.92          | 0.97       | 0.94         | 1,096,160   | 0.94     | 0.97            | 0.98            |
|                        | 6         | 0.60          | 0.19       | 0.29         | 59,336      | 0.32     | 0.19            | 1.00            |
|                        | 7         | 0.74          | 0.75       | 0.75         | 203,187     | 0.68     | 0.75            | 0.98            |
|                        | 8         | 0.86          | 0.91       | 0.88         | 115,106     | 0.82     | 0.91            | 0.99            |
|                        | 9         | 0.35          | 0.60       | 0.44         | 43,080      | 0.49     | 0.60            | 0.99            |
|                        | 10        | 0.92          | 0.53       | 0.66         | 145,587     | 0.66     | 0.53            | 1.00            |
|                        | 11        | 0.26          | 0.41       | 0.32         | 113,004     | 0.34     | 0.41            | 0.98            |
| **EfficientNet B5**    | 0         | 0.97          | 0.97       | 0.97         | 615,463     | 0.97     | 0.97            | 0.99            |
|                        | 1         | 0.94          | 0.89       | 0.91         | 1,717,184   | 0.91     | 0.89            | 0.98            |
|                        | 2         | 0.29          | 0.01       | 0.02         | 36,233      | 0.02     | 0.013           | 0.98            |
|                        | 3         | 0.99          | 0.98       | 0.98         | 1,902,580   | 0.98     | 0.98            | 1.00            |
|                        | 4         | 0.90          | 0.96       | 0.93         | 572,216     | 0.93     | 0.96            | 0.99            |
|                        | 5         | 0.93          | 0.97       | 0.95         | 1,096,160   | 0.95     | 0.97            | 0.99            |
|                        | 6         | 0.67          | 0.37       | 0.47         | 59,336      | 0.51     | 0.37            | 1.00            |
|                        | 7         | 0.82          | 0.89       | 0.85         | 203,187     | 0.74     | 0.89            | 0.98            |
|                        | 8         | 0.82          | 0.90       | 0.85         | 115,106     | 0.85     | 0.90            | 0.99            |
|                        | 9         | 0.35          | 0.62       | 0.45         | 43,080      | 0.49     | 0.62            | 0.99            |
|                        | 10        | 0.84          | 0.65       | 0.73         | 145,587     | 0.74     | 0.65            | 1.00            |
|                        | 11        | 0.23          | 0.34       | 0.27         | 113,004     | 0.30     | 0.34            | 0.98            |

UNet with EfficientNet-B5 backbone has outperformed the other two models.

### Visualisation by UNet with EfficientNet-B5
<p align="left">
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Performance/img1.png" width="200" height="180">
</p>
<p align='left'>
    <img src="https://github.com/ReehaKhan/Semantic-Segmentation/blob/main/Performance/pred1.png" width="500" height="350">
</p>
